{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versión de: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PyTorch cuenta con las librerías [TorchText](https://pytorch.org/text/stable/index.html), [TorchVision](https://pytorch.org/vision/stable/index.html), y [TorchAudio](https://pytorch.org/audio/stable/index.html) para cargar y manipular datos. En este caso, usaremos TorchVision para cargar el dataset [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "FAshionMNIST es un dataset de ropa que contiene 60,000 imágenes de entrenamiento y 10,000 imágenes de prueba. Cada imagen es de 28x28 píxeles y pertenece a una de las [10 clases de ropa](https://github.com/zalandoresearch/fashion-mnist?tab=readme-ov-file#labels). Funciona como un reemplazo directo para el dataset MNIST, que es más comúnmente usado para probar algoritmos de aprendizaje automático.\n",
    "\n",
    "Estos conjuntos de datos son subclases de `torch.utils.data.Dataset`. Cada [`Dataset` de TorchVision](https://pytorch.org/vision/stable/datasets.html) incluye dos argumentos: `transform` y `target_transform` para modificar las muestras y las etiquetas respectivamente.\n",
    "\n",
    "Las imágenes se descarga en formato PIL (Python Imaging Library) y son convertidas a tensores de PyTorch con la transformación `transforms.ToTensor()`. Tras esto, cada conjunto de datos contará en su atributo `data` con un tensor de tamaño `(n, 28, 28)` donde `n` es el número de imágenes y cada imagen es de 28x28 pixeles. En el atributo `targets` se encuentran las etiquetas de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:03<00:00, 6786540.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 722522.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 4908498.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST( # Construye un objeto FashionMNIST (subclase de torch.utils.data.Dataset)\n",
    "    root=\"data\", # directorio donde se almacenan los datos\n",
    "    train=True, # carga el conjunto de entrenamiento\n",
    "    download=True,  # descarga el conjunto de datos si no está en el directorio de datos\n",
    "    transform=ToTensor(), # ToTensor convierte la imagen en un tensor de PyTorch\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El `Dataset` se pasa como argumento a `DataLoader`, una clase que envuelve el dataset haciéndolo iterable, y que añade los procesos de *batching* (creación de lotes), *sampling* (muestreo), *shuffling* (mezcla) y carga automática de datos en múltiples procesos. Aquí definimos un tamaño de *batch* de 64, es decir, cada elemento en el iterable del dataloader devolverá un *batch* de 64 características y etiquetas.\n",
    "\n",
    "Un *batch* es un lote de muestras que se procesan en cada iteración del entrenamiento. En cada iteración, el modelo recibe un *batch* de muestras, calcula las predicciones y la función de pérdida (***loss function***), y actualiza los pesos del modelo haciendo ***backpropagation*** para minimizar la pérdida. Típicamente, se usan tamños de batch de 64, 128, 256 o 512 (potencias de 2 para que el tamaño del batch se ajuste a la memoria de la GPU y se procese de manera eficiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N(number os samples), C(color_channels), H(height), W(width)]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N(number os samples), C(color_channels), H(height), W(width)]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paquete `torch.nn` contiene las herramientas necesarias para definir redes neuronales en PyTorch. En particular, cualquier red neuronal que queramos definir debe ser una clase que herede de [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "\n",
    "Toda clase que herede de `torch.nn.Module` debe implementar dos métodos:\n",
    "- `__init__`: Constructor de la clase. Aquí se definen las capas de la red.\n",
    "- `forward`: Método que define cómo se calcula la salida de la red a partir de la entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module): # Clase que hereda de nn.Module y define la arquitectura de la red\n",
    "    def __init__(self): # Constructor de la clase\n",
    "        super().__init__() # Llama al constructor de la clase padre\n",
    "        self.flatten = nn.Flatten() # Capa de aplanamiento de la imagen (28x28 -> 784)\n",
    "        self.linear_relu_stack = nn.Sequential( # Secuencia de capas lineales y funciones de activación ReLU\n",
    "            nn.Linear(28*28, 512), # Capa de entrada con 784 entradas y 512 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa de entrada\n",
    "            nn.Linear(512, 512), # Capa oculta totalmente conectada con 512 entradas y 512 salidas\n",
    "            nn.ReLU(), # Función de activación ReLU después de la capa oculta\n",
    "            nn.Linear(512, 10) # Capa de salida con 512 entradas y 10 salidas\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # Método que define el flujo de datos a través de la red\n",
    "        x = self.flatten(x) # Aplana la imagen\n",
    "        logits = self.linear_relu_stack(x) # Pasa los datos a través de la secuencia de capas\n",
    "        return logits # Devuelve los logits (salida sin activación)\n",
    "    \n",
    "model = NeuralNetwork() # Instancia del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La capa de entrada tendrá necesariamente 784 neuronas, una por cada pixel de la imagen de entrada (recordemos que las imágenes de MNIST son de 28x28 píxeles). Para pasar los tensores de imágenes de 28x28 píxeles a un tensor de 784 píxeles, usamos la capa `Flatten`, que solo cambia la forma de los datos.\n",
    "\n",
    "La capa de salida tendrá 10 neuronas, una por cada posible dígito al que puede corresponder la imagen de entrada.\n",
    "\n",
    "Definimos en medio dos **capas ocultas (*hidden layers*)** de 512 neuronas cada una. La función de activación de las capas ocultas (para cada una de sus neuronas) es la función **ReLU** (la más común en redes neuronales).\n",
    "\n",
    "`Sequential` es un contenedor que apila módulos en el orden en que se pasan a la clase. Cada módulo se aplica a la salida del módulo anterior. En este caso, `Sequential` define la secuencia de capas de la red neuronal.\n",
    "\n",
    "`Linear` define una capa de red neuronal completamente conectada (también conocida como capa densa). Cada neurona de una capa está conectada a todas las neuronas de la capa anterior. La capa `Linear` requiere dos argumentos: el número de neuronas de entrada y el número de neuronas de salida.\n",
    "\n",
    "\n",
    "En cada ejecución de `forward`, primero se pasa la entrada a través de la capa `Flatten` para convertir la imagen de 28x28 píxeles en un tensor de 784 píxeles. Luego, la entrada se pasa a través de las capas ocultas, y se aplica la función de activación `ReLU` después de cada capa oculta. Finalmente, la salida de la última capa oculta se pasa a través de la capa de salida, que devuelve un tensor de 10 *logits*. Los *logits* son valores que no han sido normalizados y que se utilizan para calcular las probabilidades de cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para acelerar el entrenamiento, PyTorch puede aprovechar la GPU si está disponible. Para ello, se debe mover el modelo y los datos a la GPU con el método `to`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() \n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\" \n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = model.to(device) # Mueve el modelo a la GPU si está disponible\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización y entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para optimizar los parámetros del modelo, necesitamos una [**función de pérdida**](https://pytorch.org/docs/stable/nn.html#loss-functions) y un [optimizador](https://pytorch.org/docs/stable/optim.html).\n",
    "\n",
    "Definimos la función de pérdida `nn.CrossEntropyLoss`, que se utiliza comúnmente en problemas de clasificación. Esta función calcula la pérdida de entropía cruzada entre las predicciones y las etiquetas reales. La entropía cruzada es una medida de la diferencia entre dos distribuciones de probabilidad.\n",
    "\n",
    "Definimos el optimizador `optim.SGD` (descenso de gradiente estocástico) con una tasa de aprendizaje (***learning rate***) de 0.1. El optimizador ajusta los pesos del modelo en función de la pérdida calculada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # Función de pérdida\n",
    "optimizer = torch.optim.SGD( # Optimizador de descenso de gradiente estocástico\n",
    "    model.parameters(), # Parámetros del modelo a optimizar\n",
    "    lr=1e-3 # Tasa de aprendizaje\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la función `train` que realiza un paso de entrenamiento (una iteración) y la función `test` que evalúa el modelo en el conjunto de prueba.\n",
    "\n",
    "En cada iteración del bucle de entrenamiento, el modelo hace predicciones sobre el conjunto de entrenamiento (alimentado en lotes), y retropropaga el error de predicción para ajustar los parámetros del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \n",
    "    size = len(dataloader.dataset) # Número de muestras en el conjunto de datos\n",
    "    \n",
    "    model.train() # Pone el modelo en modo de entrenamiento\n",
    "    for batch_num, (X, y) in enumerate(dataloader): # Itera sobre los lotes de datos, para cada uno:\n",
    "        X, y = X.to(device), y.to(device) # Mueve el array de datos y las etiquetas al dispositivo\n",
    "\n",
    "        pred = model(X) # Genera predicciones\n",
    "        loss = loss_fn(pred, y) # Calcula la pérdida para ese lote\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # Resetea los gradientes\n",
    "        loss.backward() # Calcula el gradiente de la función de pérdida\n",
    "        optimizer.step() # Actualiza los parámetros\n",
    "\n",
    "        if batch_num % 100 == 0: # Cada 100 lotes imprime el progreso\n",
    "            loss, current = loss.item(), (batch_num + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos también una función que evalúa el modelo en el conjunto de prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval() # Pone el modelo en modo de evaluación\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad(): # Desactiva el cálculo de gradientes para el siguiente bloque de código\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() # Acumula la pérdida\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # Acumula el número de aciertos [1]\n",
    "    test_loss /= num_batches # Calcula la pérdida promedio por lote\n",
    "    correct /= size # Calcula la exactitud (número de aciertos / número total de muestras)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1]: `pred` es un tensor de forma `(batch_size, 10)` con los *logits* de cada clase. Con `pred.argmax(1)`, encontramos el índice con el valor más alto en cada fila (eje 1: columnas), o lo que es lo mismo: la clase con la probabilidad más alta para cada imagen. Comparamos las predicciones con las etiquetas reales (`(pred.argmax(1) == y)`) devolviendo un array de booleanos y lo sumamos con `sum()` para obtener el número de predicciones correctas. Para sumarlos, antes convertimos los booleanos a floats (no se usa 'int' por compatibilidad con funciones de PyTorch). El resultado es un tensor con un solo valor que contiene el número total de predicciones correctas en el *batch*; usamos `.item()` para obtener el valor numérico contenido en ese tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de entrenamiento se realiza a través de varias iteraciones (*epochs*). Una ***epoch*** es una pasada completa a través de todo el conjunto de datos de entrenamiento. Cada *epoch* se divide en lotes, y el modelo se entrena en cada lote. Después de cada *epoch*, evaluamos el modelo en el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299500  [   64/60000]\n",
      "loss: 2.289395  [ 6464/60000]\n",
      "loss: 2.268721  [12864/60000]\n",
      "loss: 2.274719  [19264/60000]\n",
      "loss: 2.246110  [25664/60000]\n",
      "loss: 2.224679  [32064/60000]\n",
      "loss: 2.239322  [38464/60000]\n",
      "loss: 2.199454  [44864/60000]\n",
      "loss: 2.195006  [51264/60000]\n",
      "loss: 2.166510  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.3%, Avg loss: 2.158978 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.166428  [   64/60000]\n",
      "loss: 2.154423  [ 6464/60000]\n",
      "loss: 2.098995  [12864/60000]\n",
      "loss: 2.128243  [19264/60000]\n",
      "loss: 2.062596  [25664/60000]\n",
      "loss: 2.009990  [32064/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;66;03m# Número de muestras en el conjunto de datos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# Pone el modelo en modo de entrenamiento\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_num, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader): \u001b[38;5;66;03m# Itera sobre los lotes de datos, para cada uno:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Mueve el array de datos y las etiquetas al dispositivo\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(X) \u001b[38;5;66;03m# Genera predicciones\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marcos\\miniconda3\\envs\\env1-py311\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Marcos\\miniconda3\\envs\\env1-py311\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Marcos\\miniconda3\\envs\\env1-py311\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Marcos\\miniconda3\\envs\\env1-py311\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Marcos\\miniconda3\\envs\\env1-py311\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Marcos\\miniconda3\\envs\\env1-py311\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marcos\\miniconda3\\envs\\env1-py311\\lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10 # Número de epochs\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Accuracy: 71.1%, Avg loss: 0.783695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardando y cargando el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entrenamiento de un modelo puede llevar mucho tiempo. Una vez un modelo está entrenado puede guardarse para moverlo a otro dispositivo, reutilizarlo, o continuar el entrenamiento más tarde.\n",
    "\n",
    "Normalmente, se guarda un modelo entrenado en un archivo como un diccionario de Python, que contiene todos los parámetros y metadatos necesarios para reanudar el entrenamiento y hacer predicciones.\n",
    "\n",
    "PyTorch tiene dos formas de guardar modelos: el método `save` y el método `load`. El método `save` guarda un modelo en un archivo, mientras que el método `load` carga un modelo de un archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cargar un modelo guardado, primero se crea una instancia de la clase del modelo y luego se llama al método `load_state_dict` para cargar los parámetros (pesos y sesgos de cada neurona) desde el fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device) # Creamos una instancia del modelo (con su arquitectura) y la movemos al dispositivo\n",
    "model.load_state_dict(torch.load(\"model.pth\")) # Cargamos los parámetros guardados del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando el modelo para hacer predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [ # Clases de FashionMNIST\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval() # Establece el modelo en modo de evaluación\n",
    "x, y = test_data[0][0], test_data[0][1] # Obtiene una imagen de prueba y su etiqueta\n",
    "with torch.no_grad(): # Deshabilita el cálculo de gradientes\n",
    "    x = x.to(device) # Movemos la imagen al dispositivo\n",
    "    pred = model(x) # Obtenemos las predicciones\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y] # Obtenemos la clase predicha y la clase real\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
